version: '3'

# source: https://github.com/botleg/swarm-logging/blob/master/docker-stack.yml
# source: https://github.com/bvis/docker-prometheus-swarm/blob/master/docker-compose.logging.yml

networks:
  monitor_monitoring:
    external:
      name: monitor_monitoring

# NOTE: scope
# source: https://docs.docker.com/engine/extend/plugins_volume/#volumedrivercapabilities
# Supported scopes are global and local.
# Any other value in Scope will be ignored,
# and local is used.
# Scope allows cluster managers to handle the volume in different ways.
# For instance, a scope of global,
# signals to the cluster manager that it only needs to
# create the volume once instead of on each Docker host.
# More capabilities may be added in the future.
volumes:
  esdata:
    driver: local

services:
  elasticsearch:
    build:
      context: container/elasticsearch
    # image: docker.elastic.co/elasticsearch/elasticsearch:5.6.1
    ports:
      - '9200:9200'
      - '9300:9300'
    networks:
      - default
      - monitor_monitoring
    environment:
      ES_JAVA_OPTS: '-Xms256m -Xmx256m'
      # NOTE: https://www.elastic.co/products/x-pack
      # Security for elasticsearch
      # For this demo, we will disable the X-Pack.
      xpack.security.enabled: 'false'
      xpack.monitoring.enabled: 'false'
      xpack.graph.enabled: 'false'
      xpack.watcher.enabled: 'false'
    volumes:
      - esdata:/usr/share/elasticsearch/data
    # TODO: Enable this
    # ulimits:
    #   memlock:
    #     soft: -1
    #     hard: -1
    #   nofile:
    #     soft: 65536
    #     hard: 65536
    # cap_add:
    #   - IPC_LOCK
    deploy:
      replicas: 1
      resources:
        limits:
          cpus: '2'
          memory: 512M
        reservations:
          cpus: '0.5'
          memory: 256M
      placement:
        constraints:
          - node.hostname == node-01

      # NOTE: example - https://github.com/kazgurs/elasticsearch/blob/4e60ae2c0a968812d333e44ea6d2c981747736f5/README.md
      # NOTE: https://github.com/mobz/elasticsearch-head#running-with-built-in-server
      head:
        # container_name: head
        image: mobz/elasticsearch-head:5
        depends_on:
          - elasticsearch
        networks:
          - default
          - monitor_monitoring
        ports:
          - 9100:9100
        deploy:
          replicas: 1
          placement:
            constraints:
              - node.hostname == node-01

  # logstash:
  #   # image: docker.elastic.co/logstash/logstash:5.3.2
  #   image: docker.elastic.co/logstash/logstash:5.6.1
  #   networks:
  #     - default
  #     - monitor_monitoring
  #   # ----------------------------------------------------
  #   # source: https://github.com/moby/moby/issues/20370
  #   # ----------------------------------------------------
  #   # Though this is an interesting use-case,
  #   # It is not easy to achieve (unless we deploy special tricks).
  #   # Reason : Name resolution between containers (via container name, links, aliases)
  #   # are handled by the embedded DNS server
  #   # (or the expanded server) in the context
  #   # of the logical network formed between the containers.
  #   # These networks are in isolated space managed and
  #   # handled by the network driver.
  #   # In some cases (such as overlay driver),
  #   # the isolated network space is not even
  #   # accessible from the host network namespace directly.
  #   # Since the docker daemon operates in the host network stack,
  #   # it may not even be able to access the service exposed
  #   # within an application network stack.
  #   # We could deploy some special tricks such
  #   # as operating the daemon in container's network stack.
  #   # But it is trickier and complex.
  #   # The simpler alternative is to
  #   # do port-mapping and access the mapped service from the docker daemon on the host-networking stack.
  #   # ----------------------------------------------------
  #   # The solution was to publish ports:
  #   # "127.0.0.1:12201:12201/udp"
  #   # instead of only 12201:12201.
  #   # My problem is solved.
  #   # Now I only have the problem that conntrack
  #   # cashes the connection even after the receiving
  #   # container dies.
  #   # ----------------------------------------------------
  #   # DISABLED # ports:
  #   # DISABLED #   - '5000:5000'
  #   # DISABLED #   - '5514:5514'
  #   # ----------------------------------------------------
  #   # NOTE: By default, the logging API attempts to bind to `tcp:9600`. If this port is already in use by another Logstash
  #   # NOTE: # The port to listen on for filebeat connections. port => 5044
  #   # ----------------------------------------------------
  #   ports:
  #     # - '127.0.0.1:5044:5044'
  #     # - '127.0.0.1:9600:9600'
  #     - '5044:5044'
  #     - '9600:9600'
  #   environment:
  #     LS_JAVA_OPTS: "-Xmx256m -Xms256m"
  #     # NOTE: Enable JMX Remote
  #     # LS_JAVA_OPTS: "-Xmx256m -Xms256m -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.port=18080 -Dcom.sun.management.jmxremote.rmi.port=18080 -Djava.rmi.server.hostname=192.168.99.103 -Dcom.sun.management.jmxremote.local.only=false"
  #   volumes:
  #     - ./logstash/logstash.conf:/usr/share/logstash/pipeline/logstash.conf
  #   depends_on:
  #     - elasticsearch
  #   deploy:
  #     replicas: 1
  #     resources:
  #       limits:
  #         cpus: '2'
  #         memory: 512M
  #       reservations:
  #         cpus: '.5'
  #         memory: 256M
  #     update_config:
  #       parallelism: 1
  #       delay: 30s
  #     placement:
  #       constraints:
  #         - node.hostname == node-01

  # logspout:
  #   image: bekt/logspout-logstash
  #   networks:
  #     - default
  #     - monitor_monitoring
  #   environment:
  #     ROUTE_URIS: 'logstash://logstash:5000'
  #     DOCKER_LABELS: "true"
  #   expose:
  #     - 8000
  #   ports:
  #     - '0.0.0.0:8000:80'
  #   # We also need to create a volume for the Docker socket,
  #   # /var/run/docker.sock.
  #   # This lets the container to attach to the docker daemon
  #   # in the host and collect all the logs.
  #   volumes:
  #     - '/var/run/docker.sock:/var/run/docker.sock'
  #   depends_on:
  #     - logstash
  #   deploy:
  #     mode: global
  #     restart_policy:
  #       condition: on-failure
  #       delay: 60s
  #     resources:
  #       limits:
  #         cpus: '0.25'
  #         memory: 64M
  #       reservations:
  #         cpus: '0.25'
  #         memory: 32M


  # kibana:
  #   # image: docker.elastic.co/kibana/kibana:5.3.2
  #   image: docker.elastic.co/kibana/kibana:5.6.1
  #   networks:
  #     - default
  #     - monitor_monitoring
  #   # The router mesh feature will then let us access
  #   # kibana from port 8088 of any host in the swarm.
  #   expose:
  #     - 5601
  #   ports:
  #     - '5601:5601'
  #   depends_on:
  #     - elasticsearch
  #   environment:
  #     ELASTICSEARCH_URL: 'http://elasticsearch:9200'
  #     XPACK_SECURITY_ENABLED: 'false'
  #     XPACK_MONITORING_ENABLED: 'false'
  #   deploy:
  #     replicas: 1
  #     resources:
  #       limits:
  #         cpus: '0.25'
  #         memory: 256M
  #       reservations:
  #         cpus: '0.25'
  #         memory: 128M
  #     placement:
  #       constraints:
  #         - node.hostname == node-03
