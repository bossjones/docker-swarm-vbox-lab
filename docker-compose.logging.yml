version: '3'

# source: https://github.com/botleg/swarm-logging/blob/master/docker-stack.yml
# source: https://github.com/bvis/docker-prometheus-swarm/blob/master/docker-compose.logging.yml

networks:
  logging:
  monitor_monitoring:
    external: true

volumes:
  esdata:
    driver: local

services:
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:5.3.2
    ports:
      - '9200:9200'
      - '9300:9300'
    networks:
      - logging
      - monitor_monitoring
    environment:
      ES_JAVA_OPTS: '-Xms256m -Xmx256m'
      # NOTE: https://www.elastic.co/products/x-pack
      # Security for elasticsearch
      # For this demo, we will disable the X-Pack.
      xpack.security.enabled: 'false'
      xpack.monitoring.enabled: 'false'
      xpack.graph.enabled: 'false'
      xpack.watcher.enabled: 'false'
    volumes:
      - esdata:/usr/share/elasticsearch/data
    deploy:
      replicas: 1
      placement:
        constraints:
          - node.hostname == node-01

  logstash:
    image: docker.elastic.co/logstash/logstash:5.3.2
    networks:
      - logging
      - monitor_monitoring
    # ----------------------------------------------------
    # source: https://github.com/moby/moby/issues/20370
    # ----------------------------------------------------
    # Though this is an interesting use-case,
    # It is not easy to achieve (unless we deploy special tricks).
    # Reason : Name resolution between containers (via container name, links, aliases)
    # are handled by the embedded DNS server
    # (or the expanded server) in the context
    # of the logical network formed between the containers.
    # These networks are in isolated space managed and
    # handled by the network driver.
    # In some cases (such as overlay driver),
    # the isolated network space is not even
    # accessible from the host network namespace directly.
    # Since the docker daemon operates in the host network stack,
    # it may not even be able to access the service exposed
    # within an application network stack.
    # We could deploy some special tricks such
    # as operating the daemon in container's network stack.
    # But it is trickier and complex.
    # The simpler alternative is to
    # do port-mapping and access the mapped service from the docker daemon on the host-networking stack.
    # ----------------------------------------------------
    # The solution was to publish ports:
    # "127.0.0.1:12201:12201/udp"
    # instead of only 12201:12201.
    # My problem is solved.
    # Now I only have the problem that conntrack
    # cashes the connection even after the receiving
    # container dies.
    # ----------------------------------------------------
    # DISABLED # ports:
    # DISABLED #   - '5000:5000'
    # DISABLED #   - '5514:5514'
    # ----------------------------------------------------
    # NOTE: By default, the logging API attempts to bind to `tcp:9600`. If this port is already in use by another Logstash
    # NOTE: # The port to listen on for filebeat connections. port => 5044
    # ----------------------------------------------------
    ports:
      - '127.0.0.1:5044:5044'
      - '127.0.0.1:9600:9600'
    volumes:
      - ./logstash/logstash.conf:/usr/share/logstash/pipeline/logstash.conf
    depends_on:
      - elasticsearch
    deploy:
      replicas: 1

  logspout:
    image: bekt/logspout-logstash
    networks:
      - logging
      - monitor_monitoring
    environment:
      ROUTE_URIS: 'logstash://logstash:5000'
      DOCKER_LABELS: "true"
    ports:
      - '0.0.0.0:8000:80'
    # We also need to create a volume for the Docker socket,
    # /var/run/docker.sock.
    # This lets the container to attach to the docker daemon
    # in the host and collect all the logs.
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
    depends_on:
      - logstash
    deploy:
      mode: global
      restart_policy:
        condition: on-failure
        delay: 60s
      resources:
        limits:
          cpus: '0.25'
          memory: 64M
        reservations:
          cpus: '0.25'
          memory: 32M


  kibana:
    image: docker.elastic.co/kibana/kibana:5.3.2
    networks:
      - logging
      - monitor_monitoring
    # The router mesh feature will then let us access
    # kibana from port 8088 of any host in the swarm.
    ports:
      - '8088:5601'
    depends_on:
      - elasticsearch
    environment:
      ELASTICSEARCH_URL: 'http://elasticsearch:9200'
      XPACK_SECURITY_ENABLED: 'false'
      XPACK_MONITORING_ENABLED: 'false'
    deploy:
      replicas: 1
