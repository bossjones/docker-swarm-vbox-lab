# my global config
global:
  scrape_interval:     15s # By default, scrape targets every 15 seconds.
  evaluation_interval: 15s # By default, scrape targets every 15 seconds.
  # scrape_timeout is set to the global default (10s).

  # Attach these labels to any time series or alerts when communicating with
  # external systems (federation, remote storage, Alertmanager).
  external_labels:
      monitor: 'my-project'

# DISABLED ## Load and evaluate rules in this file every 'evaluation_interval' seconds.
# DISABLED #rule_files:
# DISABLED #  - "alert.rules"
# DISABLED #  # - "first.rules"
# DISABLED #  # - "second.rules"

# source: https://github.com/juliusv/prometheus_docker_demo/blob/master/prometheus.yml
# A scrape configuration containing exactly one endpoint to scrape:
# Here it's Prometheus itself.
scrape_configs:
  # The job name is added as a label `job=<job_name>` to any timeseries scraped from this config.
  - job_name: 'prometheus'
    static_configs:
      - targets: ['localhost:9090']
  - job_name: 'node'
    static_configs:
      - targets: ['node-exporter:9100']
  - job_name: 'cadvisor'
    static_configs:
      - targets: ['cadvisor:8080']
  # - job_name: 'docker-engine'
  #   static_configs:
  #     - targets: ['docker-exporter:4999','']
  - job_name: 'elasticsearch'
    scrape_interval: 10s
    metrics_path: "/_prometheus/metrics"
    static_configs:
      - targets: ['elasticsearch:9200']
      # - node1:9200
      # - node3:9200

  # ± |feature-logstash-updates ✓| → docker exec -it monitor_docker-exporter.5ufz0uzbexqjjhwegq6jbvdls.uu9ggcjwko7u765cb2bc7q0oi  /bin/sh -c "nslookup tasks.docker-exporter"
  # nslookup: can't resolve '(null)': Name does not resolve
  # Name:      tasks.docker-exporter
  # Address 1: 10.0.1.17 7d37ca585e04
  # Address 2: 10.0.1.18 monitor_docker-exporter.9hwbipkvjiecnpubxt3mu2iqv.zdrpcckmwh8hqrbs7gu65xlug.monitor_default
  # Address 3: 10.0.1.19 monitor_docker-exporter.s3nyed5459b8g0m7dt1la1emk.qwqq1qd9m16a3p8woootkskd6.monitor_default
  # Address 4: 10.0.1.20 monitor_docker-exporter.e6xnk8nq5hzisuvuxynhmja29.mifjwifoxz8dgajqu1h8mbwxd.monitor_default
  # source: https://medium.com/@basi/docker-swarm-metrics-in-prometheus-e02a6a5745a
  # NOTE: We use dns to hit each of the active containers exporting docker stats
  - job_name: 'docker-exporter'
    dns_sd_configs:
    - names:
      - 'tasks.docker-exporter'
      type: 'A'
      port: 4999

  # - job_name: 'zipkin'
  #   scrape_interval: 5s
  #   metrics_path: '/prometheus'
  #   static_configs:
  #     - targets: ['zipkin:9411']
  #   metric_relabel_configs:
  #     # Response code count
  #     - source_labels: [__name__]
  #       regex: '^status_(\d+)_(.*)$'
  #       replacement: '${1}'
  #       target_label: status
  #     - source_labels: [__name__]
  #       regex: '^status_(\d+)_(.*)$'
  #       replacement: '${2}'
  #       target_label: path
  #     - source_labels: [__name__]
  #       regex: '^status_(\d+)_(.*)$'
  #       replacement: 'http_requests_total'
  #       target_label: __name__
  #     # Response time, pending histogram from https://github.com/openzipkin/zipkin/pull/1609
  #     - source_labels: [__name__]
  #       regex: '^response_(.*)$'
  #       replacement: '${1}'
  #       target_label: path
  #     - source_labels: [__name__]
  #       regex: '^response_(.*)$'
  #       replacement: 'http_request_duration_milliseconds'
  #       target_label: __name__

    # Override the global default and scrape targets from this job every 5 seconds.
    # scrape_interval: 5s

    # metrics_path defaults to '/metrics'
    # scheme defaults to 'http'.

    # static_configs:
    #      - targets: ['localhost:9090','cadvisor:8080','node-exporter:9100']
